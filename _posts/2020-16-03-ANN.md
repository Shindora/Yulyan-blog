---
layout: post
title:  "Artificial Neural Networks"
date:   2020-03-16 21:00:00
img:   /assets/ann/simpleNN.png
categories: ANN
---
<h1 style="text-align: justify;"><span style="color: #3366ff; background-color: #ffcc99;"><strong>1-Artificial Neural Networks</strong></span></h1>  

# NỘI DUNG
1. [Neuron](#neuron)
2. [Activation function](#1)
3. [Neural Network hoạt động như thế nào?](#2)  
4. [Neural Network học như thế nào?](#3)
5. [Gradient Descent](#4)
6. [Stochastic Gradient Descent (SGD)](#5)
7. [Backpropagation](#6)

## 1.Neuron <a name="neuron"></a>  
   Deep Learning đã trở thành một chủ đề nóng trong vài năm trở lại đây, nó là lĩnh vực được ngày càng nhiều sinh viên Việt Nam tìm hiểu và nghiên cứu. Vậy có gì đặc biệt và khác gì với Machine Learning? Chúng ta hãy tìm hiều trong bài viết này..  
 _(Do là bài viết đầu tiên nên sẽ có sai xót, xin chân thành cảm ơn những lời đóng góp nhận xét của mọi người.)_  
 
<hr>
<div class="imgcap">
<div >
    <img src="https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/image_data/2020-17-03-simpleNN.png" width = "800">
</div>
<div class="thecap"><p style="text-align: center;"><em>H&igrave;nh. M&ocirc; h&igrave;nh Neural Network đơn giản.</em></p></div>
</div>
<hr> 
  Mỗi neural network thường gồm 3 phần chính: input layer, hidden layers (có thể có 1 hoặc nhiều lớp), output layer.<br>
  Lớp hidden layers (xanh dương) nghe có vẻ bí ẩn nhưng thực ra nó chỉ là ["not an input or an output"](http://neuralnetworksanddeeplearning.com/chap1.html#the_architecture_of_neural_networks).Nó nhận tín hiệu (signals) từ lớp đầu vào (input layer) . Có thể hiểu input layer giống như các giác quan của chúng ta như thính giác, thị giác, xúc giác...<br>
 Vậy những tín hiệu gì nhận được từ input layer? Đó là những biến độc lập  (**independent variables**). Nó lan truyền qua khớp thần kinh hay synapes (những mũi tên màu xanh), qua hidden layers rồi cho ra giá trị output.<br>
Sự khác biệt chính giữa neural sinh học và nhân tạo là mức độ kiểm soát giá trị đầu vào và các biến độc lập.Con người ta không thế nhận biết được mức độ hôi thối, tiếng vang bên tai hay sự tồi tệ khi rớt môn...  <br>  
 __Giá trị đầu vào của neural network__  
 Đối với giá trị của những biến độc lập, ta phải chuẩn hóa (__standardize__) hoặc (__normalize__). Điều này giúp cho neural network dễ xử lý chúng hơn.<br>  
 __Giá trị đầu ra__
 <hr>
<div class="imgcap">
<div >
    <img src="https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/ann/outputNN.png" width = "800">
</div>
<div class="thecap"><p style="text-align: center;"><em>H&igrave;nh. Output.</em></p></div>
</div>
<hr>   
 Giá trị đầu ra có thể là:
 * liên tục (continuous): tiền lương, doanh thu công ty...
 * nhị  phân (binary): có hoặc không
 * phân loại (categorical): biểu hiện thời tiết như mưa,gió,mây... <br>
__Trọng số__
<hr>
<div class="imgcap">
<div >
    <img src="https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/ann/synapse.png" width = "800">
</div>
<div class="thecap"><p style="text-align: center;"><em>H&igrave;nh. Synapse.</em></p></div>
</div>
<hr>   
 Mỗi synapse là một trọng số (**weight**).  
 Trọng số là hệ số then chốt trong quá trình hoạt động của neural network.<br>
 Dựa vào trọng số mà neural có thể xác định được thông tin nào quan trọng, thông tin nào không quan trọng.<br>
 Trọng số là thứ ta điều chỉnh trong quá trình học, khác với quá trình học của con người. <br>
   
## 2.Activation function <a name="1"></a>
 Ở trong hidden layers có gì?<br>
["*They introduce non-linear properties to our Network.Their main purpose is to convert a input signal of a node in a A-NN to an output signal*."](https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f)<br>
<hr>
<div class="imgcap">
<div >
    <img src="https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/ann/activationfunction.png" width = "800">
</div>
<div class="thecap"><p style="text-align: center;"><em>H&igrave;nh. Activation function.</em></p></div>
</div>
<hr>  
<br>  
 Ta tính tổng các tích vô hướng (sum of products) của 2 vector input X và trọng số w. Sau đó áp dụng activation function $\phi(x)$ để được đầu ra của lớp này.<br>  
 Tưởng tượng nếu trọng số, tín hiệu đầu vào như cá và dầu, hidden layers sẽ là cái chảo,thì activation function là lửa và đầu ra sẽ là món cá  chiên .<br>  
 **Tại sao hàm activation phải là hàm phi tuyến tính?**<br>
 Vì nếu hàm activvation là hàm tuyến tính thì neural network của ta sẽ giống như mô hình hồi quy tuyến tính (linear regression), sẽ bị giới hạn khả năng học cũng như kém hiệu quả đối với dữ liệu phức tạp.<br>
 **Một số hàm activation thường dùng**<br>
 * __Hàm threshold__
 <hr>
 <div class="imgcap">
 <div >
    <img src="https://miro.medium.com/max/1584/1*gZcS7y482H30lmKfHnU3qw.png" width = "800">
 </div>
 <div class="thecap"><p style="text-align: center;"><em>H&igrave;nh.Hàm threshold.</em></p></div>
 </div>
 <hr>  
 <br>
 Đây là hàm activation (có lẽ đơn giản nhất). Nếu tổng trọng số là âm thì sẽ trả về 0, ngược lại trả về 1. <br>

 * __Hàm sigmoid__
 <hr>
 <div class="imgcap">
 <div >
    <img src="https://miro.medium.com/max/1739/1*6r2gdcmnpTFfeOJ4Te6zvA.png" width = "800">
 </div>
 <div class="thecap"><p style="text-align: center;"><em>H&igrave;nh.Hàm sigmoid.</em></p></div>
 </div>
 <hr>  
 <br>
 $\varphi (x)=\frac{1}{1+e^{-x}}$
 Hàm sigmoid hiện tại đã bị hạn chế dùng so với lúc trước vì có thể gây ra hiện tượng Vanishing gradient (Khó tối ưu tham số ở các layers về cuối) vì khi lấy đạo hàm của hàm mũ sẽ dẫn tích khó tối ưu ở các layers sau.<br>  

 * __Hàm Rectifier__
 <hr>
 <div class="imgcap">
 <div >
    <img src="https://miro.medium.com/max/2388/1*ajNt4kcnIXZ6LYb_CeW8Ug.png" width = "800">
 </div>
 <div class="thecap"><p style="text-align: center;"><em>H&igrave;nh.Hàm Rectifier.</em></p></div>
 </div>
 <hr>  
 <br>
 Hàm Rectifier được sử dụng nhiều hiện nay vì đơn giản. Giúp quá trình học nhanh hơn nhiều ([Ilya Sutskever](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf).
 Tuy nhiên khi sử dụng ReLU chúng ta gặp phải một hiện tượng đó là Dying ReLUs, khi một số neuron sẽ chỉ cho ra giá trị là 0 trong quá trình trainning. Cụ thể khi weights của neuron được update và weighted sum của các inputs của neuron nhỏ hơn 0 dẫn đến output = 0 và gradient = 0. Nếu learning rate lớn, một lượng lớn neurons của network có thể sẽ die. ([Viblo](https://viblo.asia/p/vanishing-exploding-gradients-problems-in-deep-neural-networks-part-2-ORNZqPEeK0n)) <br>  

 * __Hàm tanh__
 <hr>
 <div class="imgcap">
 <div >
    <img src="https://miro.medium.com/max/2156/1*aytYJ0uqNC1yhBzzAjns4A.png" width = "800">
 </div>
 <div class="thecap"><p style="text-align: center;"><em>H&igrave;nh.Hàm tanh.</em></p></div>
 </div>
 <hr>  
 <br>
 Nhìn có vẻ giống hàm sigmoid bởi vì nó là bản scaled của hàm sigmoid.Điểm lợi của hàm tanh là khi đầu vào âm sẽ được ánh xạ thành âm mạnh (strongly negative) và đầu vào 0 sẽ được ánh giá giá trị lân cận 0. Hàm tanh thường được dùng trong mô hình phân lớp nhị phân.([Sagar Sharma](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)). <br>


 
## 3.Neural Network hoạt động như thế nào? <a name="2"></a>
This is a sub paragraph, formatted in heading 3 style

## 4.Neural Network học như thế nào? <a name="3"></a>  

## 5.Gradient Descent <a name="4"></a>  

## 6.Stochastic Gradient Descent (SGD) <a name="5"></a>  

## 7.Backpropagation <a name="6"></a> 

[jekyll]:      http://jekyllrb.com
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-help]: https://github.com/jekyll/jekyll-help
