---
layout: post
title:  "Artificial Neural Networks"
date:   2020-03-16 21:00:00
img:   /assets/ann/simpleNN.png
categories: ANN
---
<h1 style="text-align: justify;"><span style="color: #3366ff; background-color: #ffcc99;"><strong>1-Artificial Neural Networks</strong></span></h1>  

# NỘI DUNG
1. [Neuron](#neuron)
2. [Activation function](#1)
3. [Neural Network hoạt động như thế nào?](#2)  
4. [Neural Network học như thế nào?](#3)
5. [Gradient Descent](#4)
6. [Stochastic Gradient Descent (SGD)](#5)
7. [Backpropagation](#6)

## 1.Neuron <a name="neuron"></a>  
   Deep Learning đã trở thành một chủ đề nóng trong vài năm trở lại đây, nó là lĩnh vực được ngày càng nhiều sinh viên Việt Nam tìm hiểu và nghiên cứu. Vậy có gì đặc biệt và khác gì với Machine Learning? Chúng ta hãy tìm hiều trong bài viết này..  
 _(Do là bài viết đầu tiên nên sẽ có sai xót, xin chân thành cảm ơn những lời đóng góp nhận xét của mọi người.)_  
 
<hr>
<div class="imgcap">
<div >
    <img src="https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/image_data/2020-17-03-simpleNN.png" width = "800">
</div>
<div class="thecap"><p style="text-align: center;"><em>H&igrave;nh. M&ocirc; h&igrave;nh Neural Network đơn giản.</em></p></div>
</div>
<hr> 
  Mỗi neural network thường gồm 3 phần chính: input layer, hidden layers (có thể có 1 hoặc nhiều lớp), output layer.<br>
  Lớp hidden layers (xanh dương) nghe có vẻ bí ẩn nhưng thực ra nó chỉ là ["not an input or an output"](http://neuralnetworksanddeeplearning.com/chap1.html#the_architecture_of_neural_networks).Nó nhận tín hiệu (signals) từ lớp đầu vào (input layer) . Có thể hiểu input layer giống như các giác quan của chúng ta như thính giác, thị giác, xúc giác...<br>
 Vậy những tín hiệu gì nhận được từ input layer? Đó là những biến độc lập  (**independent variables**). Nó lan truyền qua khớp thần kinh hay synapes (những mũi tên màu xanh), qua hidden layers rồi cho ra giá trị output.<br>
Sự khác biệt chính giữa neural sinh học và nhân tạo là mức độ kiểm soát giá trị đầu vào và các biến độc lập.Con người ta không thế nhận biết được mức độ hôi thối, tiếng vang bên tai hay sự tồi tệ khi rớt môn...  <br>  
 __Giá trị đầu vào của neural network__  
 Đối với giá trị của những biến độc lập, ta phải chuẩn hóa (__standardize__) hoặc (__normalize__). Điều này giúp cho neural network dễ xử lý chúng hơn.<br>  
 __Giá trị đầu ra__
 <hr>
<div class="imgcap">
<div >
    <img src="https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/ann/outputNN.png" width = "800">
</div>
<div class="thecap"><p style="text-align: center;"><em>H&igrave;nh. Output.</em></p></div>
</div>
<hr>   
 Giá trị đầu ra có thể là:
 * liên tục (continuous): tiền lương, doanh thu công ty...
 * nhị  phân (binary): có hoặc không
 * phân loại (categorical): biểu hiện thời tiết như mưa,gió,mây... <br>
__Trọng số__
<hr>
<div class="imgcap">
<div >
    <img src="https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/ann/synapse.png" width = "800">
</div>
<div class="thecap"><p style="text-align: center;"><em>H&igrave;nh. Synapse.</em></p></div>
</div>
<hr>   
 Mỗi synapse là một trọng số (**weight**).  
 Trọng số là hệ số then chốt trong quá trình hoạt động của neural network.<br>
 Dựa vào trọng số mà neural có thể xác định được thông tin nào quan trọng, thông tin nào không quan trọng.<br>
 Trọng số là thứ ta điều chỉnh trong quá trình học, khác với quá trình học của con người. <br>
   
## 2.Activation function <a name="1"></a>
 Ở trong hidden layers có gì?<br>
["*They introduce non-linear properties to our Network.Their main purpose is to convert a input signal of a node in a A-NN to an output signal*."](https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f)<br>
<hr>
<div class="imgcap">
<div >
    <img src="https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/ann/activationfunction.png" width = "800">
</div>
<div class="thecap"><p style="text-align: center;"><em>H&igrave;nh. Activation function.</em></p></div>
</div>
<hr>  
<br>  
<p>Ta t&iacute;nh tổng c&aacute;c t&iacute;ch v&ocirc; hướng (sum of products) của 2 vector input X v&agrave; trọng số w. Sau đ&oacute; &aacute;p dụng activation function <img src="http://www.sciweavers.org/tex2img.php?eq=%5Cphi%28x%29&amp;bc=White&amp;fc=Black&amp;im=bmp&amp;fs=12&amp;ff=arev&amp;edit=0" alt="\phi(x)" width="32" height="15" align="center" border="0" /> để được đầu ra của lớp n&agrave;y.</p>
<p>&nbsp;</p>  
 Tưởng tượng nếu trọng số, tín hiệu đầu vào như cá và dầu, hidden layers sẽ là cái chảo,thì activation function là lửa và đầu ra sẽ là món cá  chiên .<br>  
 **Tại sao hàm activation phải là hàm phi tuyến tính?**<br>
 Vì nếu hàm activvation là hàm tuyến tính thì neural network của ta sẽ giống như mô hình hồi quy tuyến tính (linear regression), sẽ bị giới hạn khả năng học cũng như kém hiệu quả đối với dữ liệu phức tạp.<br>
 **Một số hàm activation thường dùng**<br>
 * __Hàm threshold__
 <hr>
 <div class="imgcap">
 <div >
    <img src="https://miro.medium.com/max/1584/1*gZcS7y482H30lmKfHnU3qw.png" width = "800">
 </div>
 <div class="thecap"><p style="text-align: center;"><em>H&igrave;nh.Hàm threshold.</em></p></div>
 </div>
 <hr>  
 <br>
 Đây là hàm activation (có lẽ đơn giản nhất). Nếu tổng trọng số là âm thì sẽ trả về 0, ngược lại trả về 1. <br>

 * __Hàm sigmoid__
 <hr>
 <div class="imgcap">
 <div >
    <img src="https://miro.medium.com/max/1739/1*6r2gdcmnpTFfeOJ4Te6zvA.png" width = "800">
 </div>
 <div class="thecap"><p style="text-align: center;"><em>H&igrave;nh.Hàm sigmoid.</em></p></div>
 </div>
 <hr>  
 <br>
![](http://www.sciweavers.org/upload/Tex2Img_1584501924/render.png)
 <br>
 Hàm sigmoid hiện tại đã bị hạn chế dùng so với lúc trước vì có thể gây ra hiện tượng Vanishing gradient (Khó tối ưu tham số ở các layers về cuối) vì khi lấy đạo hàm của hàm mũ sẽ dẫn tích khó tối ưu ở các layers sau.<br>  

 * __Hàm Rectifier__
 <hr>
 <div class="imgcap">
 <div >
    <img src="https://miro.medium.com/max/2388/1*ajNt4kcnIXZ6LYb_CeW8Ug.png" width = "800">
 </div>
 <div class="thecap"><p style="text-align: center;"><em>H&igrave;nh.Hàm Rectifier.</em></p></div>
 </div>
 <hr>  
 <br>
 Hàm Rectifier được sử dụng nhiều hiện nay vì đơn giản. Giúp quá trình học nhanh hơn nhiều ([Ilya Sutskever](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)).
 Tuy nhiên khi sử dụng ReLU chúng ta gặp phải một hiện tượng đó là Dying ReLUs, khi một số neuron sẽ chỉ cho ra giá trị là 0 trong quá trình trainning. Cụ thể khi weights của neuron được update và weighted sum của các inputs của neuron nhỏ hơn 0 dẫn đến output = 0 và gradient = 0. Nếu learning rate lớn, một lượng lớn neurons của network có thể sẽ die. ([Viblo](https://viblo.asia/p/vanishing-exploding-gradients-problems-in-deep-neural-networks-part-2-ORNZqPEeK0n)) <br>  

 * __Hàm tanh__
 <hr>
 <div class="imgcap">
 <div >
    <img src="https://miro.medium.com/max/2156/1*aytYJ0uqNC1yhBzzAjns4A.png" width = "800">
 </div>
 <div class="thecap"><p style="text-align: center;"><em>H&igrave;nh.Hàm tanh.</em></p></div>
 </div>
 <hr>  
 <br>
 Nhìn có vẻ giống hàm sigmoid bởi vì nó là bản scaled của hàm sigmoid.Điểm lợi của hàm tanh là khi đầu vào âm sẽ được ánh xạ thành âm mạnh (strongly negative) và đầu vào 0 sẽ được ánh giá giá trị lân cận 0. Hàm tanh thường được dùng trong mô hình phân lớp nhị phân.([Sagar Sharma](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)). <br>


 
## 3.Neural Network hoạt động như thế nào? <a name="2"></a>
 Ta đến với ví dụ quyết định giá trị một ngôi nhà, với đầu vào có 4 biến:
 * Diện tích
 * Số phòng
 * Khoảng cách đến thành phố 
 * Tuổi thọ ngôi nhà
<br>
Tất cả 4 biến  được liên kết với neuron qua synapse. Tuy nhiên, **không phải synapse nào cũng là  trọng số, chúng có thể có giá trị 0 hoặc khác 0**. 
![](https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/ann/NNwork1.png)
Bình thường thì ta sẽ được ngôi nhà lớn và rẻ khi xa thành phố. Đúng không? **Nhưng**, neural network suy nghĩ theo hướng cụ thể hơn khi tìm ngôi nhà có diện tích lớn nhưng không quá xa thành phố. Vì vậy, node đầu tiên tập trung vào 2 yếu tố diện tích và khoảng cách, không quan tâm đến số phòng và  tuổi thọ.
![](https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/ann/NNwork2.png)
Đây là cái mạnh của Neural network. Hãy chú ý vào neuron thứ 3. <br>
Khi ta thiết kế một bộ dữ liệu đã được huấn luyện từ trước, khi áp vào neuron này,sau khi qua vài ngàn examples, nó nghĩ  là sự kết hợp của diện tích, số phòng và tuổi là yếu tố quan trọng ảnh hưởng đến giá nhà. <br>
Tại sao có trường hợp này?
Có thể  gia đình ở ngoại ô thành phố, có nhiều thành viên đang tìm kiếm một ngôi nhà lớn, nhiều phòng, mới chứ không phải nhà cổ chẳng hạn.Do đó, qua quá trình huấn luyện, neural network có các yếu tố kể trên sẽ có giá trị. Điều này rất quan trọng trong lĩnh vực bất  động sản.<br>
![](https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/ann/NNwork3.png)
Có thể giải thích một cách tương tự đối với các neuron khác.
Qua những tương tác này, có thể thấy bản thân neural network rất linh hoạt, có thể tìm kiếm những thứ cụ thể, toàn diện xác định để huấn luyện.<br>


## 4.Neural Network học như thế nào? <a name="3"></a>  
![*Hình. Neural network học như thế nào?*](https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/ann/NNlearn.png)
Hình này gần giống như các phần trước nhưng có 1 điểm khác biệt là y và y_hat. Đó là gì?<br>
Khi các giá trị đầu vào qua synapse, vào neuron, kết hợp với hàm activation sẽ cho ra y_hat. Còn y là đầu ra có trong bộ dữ liệu (data set) , là label (đối với bài toán phân lớp), là giá trị continuous (đối với bài toán hồi quy).
Để neural network có thể học, cần có sự so sánh giữa y và y_hat.
Cách so sánh đó gọi là hàm chi phí  ([cost function](https://towardsdatascience.com/coding-deep-learning-for-beginners-linear-regression-part-2-cost-function-49545303d29f)). Cost function cho biết độ lỗi (error) trong dự đoán. Mục đích chủ yếu là giảm cost fuction, đưa y và y_hat gần nhau hơn.<br>
Ví dụ:
Dự đoán khả năng qua môn máy học của một sinh viên dựa trên các yếu tố:
 * Giờ học (giờ)
 * Hoàn thành deadline (số lần)
 * Độ khó bài thi (1-5)
 * Output (%)
![](https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/ann/ex_learn.png)
95% là y,  y_hat dựa trên kết quả đầu ra của hàm activation.<br>
Sau đó, sử dụng cost function:
![](https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/ann/costfunc.png)
Nếu có sự chênh lệnh giữa y và y_hat thì điều chỉnh trọng số, cho đến khi cost  function được tối thiểu (minimized).
**Chú ý**: Quá trình này không làm thay đổi giá trị đầu vào, chỉ có điều chỉnh trọng số.<br>


## 5.Gradient Descent <a name="4"></a>  
 Như đã đề cập ở phần bên trên, mục  tiêu của chúng ta là cần minimized cost function để nâng cao hiệu quả của mô hình. Vậy, làm thế nào để minimized cost function?<br>
![](https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/ann/gd1.png)
Có 2 cách tiếp cận vấn đề này: brute-force và gradient descent.
Hiểu một cách đơn giản là brute-force sẽ kiểm tra mọi điểm có thể và chọn nơi tốt nhất. Trong khi đó, tư tưởng của gradient descent là thực hiện thực hiện theo từng bước, giảm từ từ cost function và cho kết quả tốt hơn sau mỗi lần lặp (iteration).
Hướng brute-force sẽ dẫn đến vấn đề rất lớn gọi là [Curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) và quá trình huấn luyện rất lâu đối với những bộ dữ liệu kích thước lớn.
Ví dụ: đối với bài toán giá trị ngôi nhà ở phần trên, nếu lớp input full-connected với hidden layer ta có 25 trọng số, có nghĩa nếu có 1000 tổ hợp (combination), sử dụng brute-force với tất cả tổ hợp ta sẽ có ![](http://www.sciweavers.org/upload/Tex2Img_1584518953/render.png)
Siêu máy tính nhanh nhất thế  giới Sunway TaihuLight (2016) có tốc độ 93 PFLOPS sẽ chạy trong trong  $$3.42x10^{50}$$ năm. Kinh khủng !!!<br>
Vì vậy, ta sẽ giải quyết vấn đề theo hướng gradient descent.
<hr>
<div class="imgcap">
<div >
    <img src="https://qph.fs.quoracdn.net/main-qimg-e17db9e2656cd2359f8a45cbe820a63c" width = "800">
</div>
<div class="thecap"><p style="text-align: center;"><em>H&igrave;nh.Gradient Descent (Nguồn: [Quora](https://www.quora.com/What-is-the-step-size-in-gradient-descent)).</em></p></div>
</div>
<hr> 
<br>
Thay vì đi qua mỗi trọng số và đánh dấu lại mỗi lần đi, ta sẽ chú ý vào góc của đồ thị biễu diễn cost function.<br>
Nếu độ dốc âm, nghĩa là bạn đang đi xuống.<br>
Tưởng tượng bạn đang nhảy từ cột nhà này sang cột nhà kia để xuống đất như trong phim kiếm hiệp vậy. Điều này làm bỏ qua một số trọng số không cần thiết, giúp làm giảm thời gian tìm trọng số tối ưu.<br>
Phần kế tiếp ta sẽ tìm hiểu chi tiết về cách hoạt động  Stochastic Gradient Descent, một phiên bản Giadient Descent phổ biến hiện nay.<br>  


## 6.Stochastic Gradient Descent (SGD) <a name="5"></a>  

## 7.Backpropagation <a name="6"></a> 

[jekyll]:      http://jekyllrb.com
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-help]: https://github.com/jekyll/jekyll-help
