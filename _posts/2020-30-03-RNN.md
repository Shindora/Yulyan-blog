---
layout: post
title: Recurrent Neural Network (RNN) 
date:   2020-03-30 21:00:00
categories: RNN
---
<h1 style="text-align: justify;"><span style="color: #3366ff; background-color: #ffcc99;"><strong>2 - Recurrent Neural Network (RNN)</strong></span></h1>  

# NỘI DUNG
1. [Ý tưởng đằng sau RNN](#1)
2. [Ứng dụng của RNN](#2)
3. [Vấn đề Vanishing Gradient ](#3)  
4. [Long Short-Term Memory (LSTM)](#4)


## 1.Ý tưởng đằng sau RNN <a name="1"></a>  
__1.1. Neural network và não bộ con người__<br>
 Như chúng ta đã biết thì các lý thuyết cơ sở của Deep Learning được hình và phát triển dựa trên sự bắt chước theo mô hình của não bộ con người.<br>
 Não người gồm có 3 phần bao gồm: đại não ([cerebrum](https://en.wikipedia.org/wiki/Cerebrum)), là hình ảnh phía dưới, tiểu não ([cerebellum](https://en.wikipedia.org/wiki/Cerebellum)) và trụ não ([brainstem](https://en.wikipedia.org/wiki/Brainstem)), được liên kết với tủy.
 ![](https://upload.wikimedia.org/wikipedia/vi/e/e1/Nao_nguoi.jpg)
 _Nguồn: [Wiki](https://vi.wikipedia.org/wiki/N%C3%A3o_ng%C6%B0%E1%BB%9Di)_<br>
__1.2. Liên hệ giữa thùy trán và RNN__<br>
   RNNs hoạt động giống như trí nhớ tạm thời vậy, chúng ta sẽ học được những hiện tượng qua một vài lần bắt gặp mà sử dụng nó vào những việc sau này.<br>
   Đối với con người, trí nhớ ngắn hạn là một trong những chức năng của thùy trán.
 <br>
__1.3. Biểu diễn mô hình RNN__<br>
  Ta bắt đầu chuyển đổi một mô hình ANN sang RNN <br>
  ![](https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/rnn/1.png)
  1. Nén lại. Các lớp vẫn nguyên, ta hãy tưởng tượng đang quan sát mô hình từ phía dưới
  ![](https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/rnn/2.png)
  2. Giản lượt bớt các mũi tên
  ![](https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/rnn/3.png)
  3. Xoay theo chiều thẳng đứng theo đúng mô hình chuẩn
  ![](https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/rnn/4.png)
  4. Thêm vào vòng lặp (temporal loop), hidden layer không chỉ đưa ra output mà còn phản hồi lại chính nó (đây là cách biểu diễn cũ).
  ![](https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/rnn/5.png)
  5. Bỏ kiếm soát vòng lặp và biểu diễn RNN theo cách mới. Bây giờ, mỗi vòng tròn không chỉ là một neuron mà còn là một lớp neuron
  ![](https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/rnn/6.png)
 
 
## 2.  Ứng dụng của RNN <a name="2"></a><br>
  __One to Many__ : RNN có một input và nhiều output
  ![](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/41_blog_image_11.png)
  Bức ảnh này ban đầu đi qua CNN để xử lý ảnh (image processing) và nhận dạng đặc trưng (feature  recognition), rồi qua RNN để máy tính cho ra một câu có nghĩa :"black and white dog jumps over bar".<br>
   Có thể ứng dụng vào các __search engine__, __eCommerce__,__social media__ như Facebook, Instagram.<br>
  __Many to One__: RNN có nhiều input và một output
  ![](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/41_blog_image_12.png)
  Một ví dụ rất quen thuộc là bài toán phân tích cảm xúc (sentiment analysis), khi một câu có nhiều từ như lời bình luận khách hàng, phản hồi của sinh viên,... ta cần một thứ có thể đánh giá được câu đó mang ý nghĩa tích cực, tiêu cực hay trung tính bao nhiêu, qua đó để có chiến lược quảng bá, sửa đổi thích hợp.<br>
   __Many to Many__: RNN có nhiều input lẫn output
   ![](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/41_blog_image_13.png)
   Ví dụ trong trường hợp này là công cụ dịch ngôn ngữ như Google Translaltor.
    ![](https://raw.githubusercontent.com/Shindora/Yulyan-blog/gh-pages/assets/rnn/gg.png)
   Có ứng dụng vào dịch máy (machine translation),text summarization,chatbots...<br>
    
## 3. Vấn đề Vanishing Gradient <a name="3"></a><br>
 __Vanishing Gradient là gì?__
  Trong nội dung bài trước có đề cập, gradient descent là thuật toán giúp tìm ra cực tiểu toàn cục (global minimum) của hàm chi phí (cost function), nhằm cho ra trọng số tối ưu, nâng cao hiệu quả của neural network.<br>
  Thuật toán này cũng tương tự trong RNN nhưng có một chút khác biệt:
  - Đầu tiên, tín hiệu truyền qua RNN, có nghĩa là tín hiệu đầu ra lúc trước dùng để làm  đầu vào cho thời gian lúc sau.<br>
  - Kế tiếp, tính hàm chi phí hay hàm lỗi tại mỗi thời gian T.<br>
  Chú ý: mỗi node xanh bây giờ không chỉ là một neuron mà còn là một hidden layer hoàn chỉnh.
 ![](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/41_blog_image_16.png)
 Bây giờ, mỗi neuron đơn lẻ tham gia vào quá trình tính toán của output để cập nhật trọng số của nó nhằm giảm thiểu độ lỗi.<br>
 Vấn đề này có liên quan đến cập nhật Wrec (weight recurring): trọng số được sử dụng để liên kết những hidden layer với chính nó trong vòng lặp.<br>
 Để nhận được X_t-3 từ X_t-2, ta nhân X_t-3 với Wrec, sau đó, để nhận được X_t-2 từ X_t-1, ta lại lần nữa nhân X_t-2 với Wrec.<br>
 __Vấn đề xảy ở đây?__<br>
  Khi ta nhân một số lớn với một số nhỏ (trị tuyệt đối nhỏ hơn 1 chẳng hạn) thì nhận được một số càng nhỏ. Nhưng ở bài ANN trước, trọng số đầu vào được khởi tạo ngẫu nhiên với giá trị lân cận 0 (nhưng khác 0). Từ đó dẫn tới gradient của ta giảm nhanh chóng và ngày càng nhỏ.
![](https://www.andreaperlato.com/img/vanishinggradient%20descent.png)
  Gradient nhỏ khiến cho ta khó cập nhật trọng số và mất thời gian cho ra kết quả.<br>
 Ví dụ: bình thường với 1000 epochs có thể cho ra trọng số tối ưu nhưng bây giờ sẽ không đủ khi ở thời điểm t-3 gradient rất nhỏ, dẫn đến hiệu ứng domino trên toàn neural network.<br>
 Tóm lại, nếu Wrec nhỏ, khả năng rất cao sẽ bị vanishing gradient.<br>
__Hướng giải quyết__<br>
- Khởi tạo trọng số sao cho khả năng xảy ra vanishing gradient được giảm thiểu.
- Có Echo State Networks được thiết kế để giải quyết vấn đề vanishing gradient.
- Sử dụng Long Short-Term Memory Networks (LSTMs).

<br>
## 4. Long Short-Term Memory <a name="4"></a><br>
 Chức năng của LSTM là thêm, xóa và điều chỉnh các luồng thông tin bằng một cơ chế gọi là cổng.
 ![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png)
  Đây là một LSTM cơ bản, nhìn có vẻ rất rối, ta sẽ tìm hiểu cơ chế hoạt động của từng phần ở phía dưới.
 ![](https://www.andreaperlato.com/img/lstms.png)
 - C_t-1 là input cho memory cell ở thời điểm t.<br>
 - h_t là output tại thời điểm t.<br>
 - X_t là input tạ thời điểm t.<br>
  Mỗi khối (block) có 3 input (X_t,h_t-1,C_t-1) và 2 output (h_t và C_t).<br>
 Chú ý: tất cả input và output không phải là một giá trị đơn lẻ mà là các vector.<br>
 __Cơ chế hoạt động của LSTM theo từng bước__<br>
- __Đầu tiên__,quyết định thông tin nào ta cho đi qua ở các ô trạng thái.Quyết định này được đưa ra bởi lớp __sigmoid__ được gọi là "__forget gate layer__" (tầng cổng quên), sigmoid là hàm activation cho ra giá trị từ 0 đến 1. 1 là giữ toàn bộ thông tin và 0 là bỏ qua toàn bộ.
  ![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)
 Ví dụ: một mô hình dự đoán từ kế tiếp dựa trên các từ trước đó của một câu. Ô trạng thái chứa giới tính của chủ ngữ, từ đó có thể cho ra đúng đại từ nhân xưng. Nhưng không phải bao giờ cũng dùng cố định một chủ ngữ, như là : Tom drives a car. He drives so fast. Khi ta gặp một chử ngữ mới, sẽ có xu hướng quên đi chủ ngữ cũ.<br>
 Forget gate layer có tác dụng lưu trữ và cập nhật thông tin khi nó thay đổi theo thời gian.
- __Kế tiếp__, quyết định thông tin mới nào được lưu trữ trong cell. Gồm 2 phần:
-- Một lớp sigmoid, được gọi là "__input gate layer__", quyết định giá trị nào được cập nhật.<br>
-- Một lớp tanh , tạo ra vector chứa trạng thái mới.<br>
 Kết hợp cả hai, tạo ra một cập nhật mới cho trạng thái.
![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)
 Theo ví dụ ở trên, ta muốn thêm giới tính cho chủ ngữ mới, thay thế cái cũ mà ta quên.<br>
- __Tiếp theo__, ta nhân trạng thái cũ (C_t-1) với f_t, quên những ta quyết định quên sớm, rồi cộng với (i_t*~C_t), tương ứng với ta muốn cập nhật trạng thái mới bao nhiêu.
![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)
 Tiếp tục ví dụ, đây là quá trình ta lược bỏ thông tin về chủ ngữ cũ và thêm vào thông tin mới.<br>
- __Cuối cùng__, quyết định output, gồm 2 phần:<br>
-- Một lớp sigmoid, quyết định phần nào của trạng thái cho vào output.<br>
-- tanh (cho ra giá trị từ -1 tới 1), và nhân nó với output của cổng sigmoid.<br>
![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)
 Khi đã có chủ ngữ, mô hình chọn động từ phù hợp theo. Ví dụ như số ít hay số nhiều từ đó quyết định được thể của động từ theo đúng ngữ pháp.<br>
 Đây chỉ là  một kiến trúc LSTM cơ bản, ngoài ra còn rất nhiều biến thể khác.<br>

**Building an simple RNN yourself**: [source](https://github.com/Shindora/Deep-Learning/blob/master/BuildingRNN.py)<br>
**Data**: [Google Stock Price dataset](https://www.kaggle.com/medharawat/google-stock-price)<br>
**Contact**:tuanvi1792k@gmail.com<br>
**TÀI LIỆU THAM KHẢO**<br>
--SuperDataScience: [https://www.superdatascience.com/blogs/the-ultimate-guide-to-recurrent-neural-networks-rnn](https://www.superdatascience.com/blogs/the-ultimate-guide-to-recurrent-neural-networks-rnn)<br>
-- Andrea Perlato:[https://www.andreaperlato.com/aipost/recurrent-neural-network-in-theory/](https://www.andreaperlato.com/aipost/recurrent-neural-network-in-theory/)<br>
-- Christopher Olah:[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

<br>
[jekyll]:      http://jekyllrb.com<br>
[jekyll-gh]:   https://github.com/jekyll/jekyll<br>
[jekyll-help]: https://github.com/jekyll/jekyll-help<br>



 

 
